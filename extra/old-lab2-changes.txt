import requests

# Use Ollama to generate a detailed response based on the retrieved answer
ollama_url = "http://localhost:11434/api/generate"
 
query = {
    "model": "llama3",
    "prompt": f"Expand on the following answer:\n\n{answer}",
    "stream": False
}
 
response = requests.post(ollama_url, json=query)
 
# Check if the request was successful
if response.status_code == 200:
    # Process the response
    if query["stream"]:
        # Handle streaming response
        for line in response.iter_lines():
            if line:
                body = json.loads(line)
                print(body.get("response", ""))
    else:
        # Handle non-streaming response
        response_data = response.json()
        print(response_data["response"])
else:
    # Print error message
    print("Error:", response.status_code, response.text)
